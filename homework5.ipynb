{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Visit the Wikipedia hyperlinks graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "from heapq import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start downloading <a href=\"https://drive.google.com/file/d/1ghPJ4g6XMCUDFQ2JPqAVveLyytG8gBfL/view\" > Wikicat hyperlink graph </a>. \n",
    "\n",
    "It is a reduced version of the one we can find on SNAP. \n",
    "\n",
    "Every row is an <b>edge</b>. The two elements of each row are the <b>nodes</b>, in particular the first is the <b> source</b>, the second represents the <b>destination</b>.\n",
    "\n",
    "So, our first goal is open and read the file with python, and split each line with the new line charachter.\n",
    "Then we take all the <i>source nodes</i> for each row, and we put them as keys into our <b>graph</b> dictionary. The values will be all the correspondent destination nodes.\n",
    "\n",
    "But we have not done! Infact our scope is to analyze the graph, in particular discovering the following informations:\n",
    "\n",
    "* If it is direct or not;\n",
    "\n",
    "* The number of nodes;\n",
    "\n",
    "* The number of edges;\n",
    "\n",
    "* The average node degree. Is the graph dense?\n",
    "\n",
    "To do this we want that our dictionary has as keys <u>all the nodes</u>, sources and destinations and for now we have just the first ones. So we add as new keys all the nodes that are just destinations, putting as values empty lists.\n",
    "\n",
    "\n",
    "Now we have the dictionary with all the nodes of our graph as keys, and as values there are all the eventual destinations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = open('wiki-topcats-reduced.txt','r') \n",
    "all_rows = F.read().split('\\n')\n",
    "\n",
    "graph = {}\n",
    "for row in all_rows:\n",
    "    row = row.split(\"\\t\")\n",
    "    if row[0] not in graph:\n",
    "        try:\n",
    "            graph[row[0]] = [row[1]]\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        graph[row[0]].append(row[1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "for l in graph.values():\n",
    "    lista+= l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in lista:\n",
    "    if node not in graph:\n",
    "        graph[node] = []\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what can we say?\n",
    "\n",
    "* We are in a case of <b>page ranking</b>. So for definition we have nodes representing sources and destinations, with edges with a particular direction. In other words, our graph has a set of edges which are <i>ordered pairs</i> of nodes, and for the graph theory we have a <b>directed graph</b>.\n",
    "\n",
    "\n",
    "* The number of nodes is <u>461193</u>. It's just the number of keys of our dictionary.\n",
    "\n",
    "\n",
    "* The number of edges is <u>2645247</u> and it's computed looking at all the lenghts of the <b>adjacency list</b>.\n",
    "\n",
    "\n",
    "* In graph theory, the <b>degree</b> (or <i>valency</i>) of a vertex of a graph is the number of edges incident to the vertex. We need the <b>average node degree</b>, so we compute the ratio between number of edges and number of nodes. It results <u>5.735661642739591</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461193"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = list(graph.keys())\n",
    "n_nodes = len(V)\n",
    "n_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645247"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_edges = 0\n",
    "for l in graph.values():\n",
    "    n_edges += len(l)\n",
    "n_edges    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avarage node degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.735661642739591"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_degree = n_edges/n_nodes\n",
    "avg_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start creating a dictionary called <b>categories</b> where for every category taken from <i>wiki-topcats-categories.txt</i> file, we have the list of all its articles. But attention! we must take into account all the categories that has a number of articles greater than <b>3500</b>. So we filter our dictionary considering the categories with more that 3500 articles. Another, we take just the articles that are also in our <b>graph</b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = open('wiki-topcats-categories.txt','r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "for line in C.readlines():\n",
    "    l = line.split(' ')\n",
    "    cat = l[0].replace(\"Category:\",\"\").replace(\";\", \"\")\n",
    "    art = l[1:]\n",
    "    art[-1] = art[-1].replace(\"\\n\",\"\")\n",
    "    if len(art) >= 3500:\n",
    "        categories[cat]= set(art).intersection(set(V))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ds055uzetaobb.cloudfront.net/image_optimizer/9e7d1e7f0beab28be5095491b4edcb51c22f9a6b.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
